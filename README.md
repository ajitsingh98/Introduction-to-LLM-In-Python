# Large Language Model From Scratch in PyTorch

Hi, In this project I have implemented LLM from scratch. It contains implementing attention mechanish from scratch to pretraining and fine-tuning LLM. Entire code is in `Python` and `Pytorch` and it assumes a solid understanding of programming in `Python` and basic understanding of `Pytorch` module.



Contents:
---
- Working with textual data
- Attention Mechanism from scratch
- GPT model from scratch
- Pretraining on Unlabled Data
- Finetune LLMs for text classification
- Finetuning with human feedback to follow instructions
- LLM in practice
  
References
---
- <a href="https://arxiv.org/abs/1706.03762"> Attention Is All You Need </a>
- <a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention">Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs</a>
- <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY"> Let's build GPT: from scratch, in code, spelled out by Andrej Karpathy</a>
